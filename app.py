import streamlit as st
import os
import json
import re
from pathlib import Path
import sqlite3 
import uuid 
import time 

from google import genai as google_genai_sdk 
from google.genai import types as google_genai_types
from google.api_core.exceptions import PermissionDenied, InvalidArgument, NotFound, GoogleAPIError

# --- Configuration ---
DOC_DIR = Path("documents") 
PDF_FILENAMES = ["tuyen_duong_sat_do_thi_hcm.pdf", "xe_dap_cong_cong_xe_dien_4_banh_va_xe_buyt_duong_song.pdf", "xe_buyt.pdf"]
GEMINI_API_KEY_FILE = Path("gemini_api_key.json")
DATABASE_PATH = Path("chat_sessions.db") 

GEMINI_MODEL_ID = "gemini-2.0-flash" 

GEMINI_CLIENT = None
UPLOADED_FILES_CACHE = {} 

# --- Database Helper Functions ---
def init_db():
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS sessions (
            id TEXT PRIMARY KEY, name TEXT NOT NULL,
            created_at INTEGER NOT NULL, last_updated_at INTEGER NOT NULL,
            pdfs_uploaded INTEGER DEFAULT 0 ) ''')
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS messages (
            id TEXT PRIMARY KEY, session_id TEXT NOT NULL, role TEXT NOT NULL,
            content TEXT NOT NULL, timestamp INTEGER NOT NULL,
            gemini_grounding_metadata_json TEXT, 
            FOREIGN KEY (session_id) REFERENCES sessions (id) ON DELETE CASCADE ) ''')
    cursor.execute('''CREATE INDEX IF NOT EXISTS idx_messages_session_id_timestamp ON messages (session_id, timestamp);''')
    conn.commit(); conn.close()

def create_new_session_db(session_name_prefix="Tr√≤ chuy·ªán m·ªõi"):
    session_id = str(uuid.uuid4()); conn = sqlite3.connect(DATABASE_PATH); cursor = conn.cursor()
    count = 0; session_name = f"{session_name_prefix}"
    while True:
        cursor.execute("SELECT COUNT(*) FROM sessions WHERE name = ?", (session_name,))
        if cursor.fetchone()[0] == 0: break
        count += 1; session_name = f"{session_name_prefix} ({count})"
    current_time = int(time.time())
    cursor.execute("INSERT INTO sessions (id, name, created_at, last_updated_at, pdfs_uploaded) VALUES (?, ?, ?, ?, ?)",
                   (session_id, session_name, current_time, current_time, 0))
    conn.commit(); conn.close(); return session_id, session_name

def get_sessions_db():
    conn = sqlite3.connect(DATABASE_PATH); cursor = conn.cursor()
    cursor.execute("SELECT id, name, last_updated_at, pdfs_uploaded FROM sessions ORDER BY last_updated_at DESC")
    sessions = [{"id": r[0], "name": r[1], "last_updated_at": r[2], "pdfs_uploaded": r[3]} for r in cursor.fetchall()]
    conn.close(); return sessions

def load_messages_db(session_id):
    conn = sqlite3.connect(DATABASE_PATH); cursor = conn.cursor()
    cursor.execute("SELECT role, content, gemini_grounding_metadata_json FROM messages WHERE session_id = ? ORDER BY timestamp ASC", (session_id,))
    messages = []
    for row in cursor.fetchall():
        msg = {"role": row[0], "content": row[1]}
        if row[2]: # gemini_grounding_metadata_json
            try: msg["gemini_grounding_metadata"] = json.loads(row[2]) 
            except json.JSONDecodeError: msg["gemini_grounding_metadata_error"] = "L·ªói parse metadata"
        messages.append(msg)
    conn.close(); return messages

def save_message_db(session_id, role, content, grounding_metadata_obj=None):
    message_id = str(uuid.uuid4()); current_time = int(time.time())
    conn = sqlite3.connect(DATABASE_PATH); cursor = conn.cursor()
    grounding_metadata_json_str = None
    if grounding_metadata_obj:
        try: grounding_metadata_json_str = json.dumps(grounding_metadata_obj)
        except TypeError: st.warning("Kh√¥ng th·ªÉ serialize grounding metadata.")
    cursor.execute("INSERT INTO messages (id, session_id, role, content, timestamp, gemini_grounding_metadata_json) VALUES (?, ?, ?, ?, ?, ?)",
                   (message_id, session_id, role, content, current_time, grounding_metadata_json_str))
    cursor.execute("UPDATE sessions SET last_updated_at = ? WHERE id = ?", (current_time, session_id))
    conn.commit(); conn.close()

def set_pdfs_uploaded_for_session_db(session_id):
    conn = sqlite3.connect(DATABASE_PATH); cursor = conn.cursor()
    cursor.execute("UPDATE sessions SET pdfs_uploaded = 1, last_updated_at = ? WHERE id = ?", (int(time.time()), session_id))
    conn.commit(); conn.close()

def rename_session_db(session_id, new_name):
    conn=sqlite3.connect(DATABASE_PATH);cursor=conn.cursor()
    try: cursor.execute("UPDATE sessions SET name = ?, last_updated_at = ? WHERE id = ?", (new_name, int(time.time()), session_id)); conn.commit(); return True
    except sqlite3.Error as e: st.error(f"L·ªói DB rename: {e}"); return False
    finally: conn.close()

def delete_session_db(session_id):
    conn=sqlite3.connect(DATABASE_PATH);cursor=conn.cursor()
    try: cursor.execute("DELETE FROM sessions WHERE id = ?", (session_id,)); conn.commit(); return True
    except sqlite3.Error as e: st.error(f"L·ªói DB delete: {e}"); return False
    finally: conn.close()

init_db()

def load_api_key():
    if GEMINI_API_KEY_FILE.exists():
        with open(GEMINI_API_KEY_FILE, 'r') as f: data = json.load(f); return data.get("GEMINI_API_KEY")
    return os.environ.get("GEMINI_API_KEY")

def save_api_key(api_key_value):
    with open(GEMINI_API_KEY_FILE, 'w') as f: json.dump({"GEMINI_API_KEY": api_key_value}, f)

@st.cache_resource
def get_gemini_client(api_key_value):
    try:
        client = google_genai_sdk.Client(api_key=api_key_value)
        client.models.list() 
        st.success("Gemini Client ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng!")
        return client
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o Gemini Client: {e}. Ki·ªÉm tra API Key.")
        return None

def upload_files_to_gemini(client, pdf_filenames_list, current_session_id):
    if current_session_id in UPLOADED_FILES_CACHE and UPLOADED_FILES_CACHE[current_session_id]:
        st.info(f"S·ª≠ d·ª•ng file ƒë√£ upload cho session {current_session_id} t·ª´ cache.")
        return UPLOADED_FILES_CACHE[current_session_id]

    uploaded_file_objects = []
    st.write("ƒêang upload t√†i li·ªáu PDF l√™n Gemini...")
    for filename in pdf_filenames_list:
        file_path_obj = DOC_DIR / filename
        if file_path_obj.exists():
            try:
                with st.spinner(f"Uploading {filename}..."):
                    st.write(f"ƒêang upload: {file_path_obj.name}")
                    gemini_file_obj = client.files.upload(file=file_path_obj) 
                    uploaded_file_objects.append(gemini_file_obj)
                    st.success(f"ƒê√£ upload: {filename} (ID: {gemini_file_obj.name})")
            except Exception as e:
                st.error(f"L·ªói upload file {filename}: {e}")
                st.error(f"Chi ti·∫øt l·ªói: {type(e).__name__} - {e}")
        else:
            st.error(f"Kh√¥ng t√¨m th·∫•y file: {file_path_obj}")
    
    if uploaded_file_objects:
        UPLOADED_FILES_CACHE[current_session_id] = uploaded_file_objects
        set_pdfs_uploaded_for_session_db(current_session_id)
    else:
        st.warning("Kh√¥ng c√≥ file PDF n√†o ƒë∆∞·ª£c upload th√†nh c√¥ng.")
    return uploaded_file_objects

def generate_gemini_response_stream(client, user_prompt_text, current_session_id, existing_chat_history):
    global UPLOADED_FILES_CACHE
    model_to_use = GEMINI_MODEL_ID 
    
    system_instruction_string = """b·∫°n l√† m·ªôt tr·ª£ l√Ω v·ªÅ giao th√¥ng c√¥ng c·ªông khu v·ª±c n·ªôi th√†nh th√†nh ph·ªë h·ªì ch√≠ minh. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c th√¥ng tin v·ªÅ giao th√¥ng c√¥ng c·ªông m·ªôt c√°ch chi ti·∫øt, n·∫øu th√¥ng tin li√™n quan cho c√¢u h·ªèi kh√¥ng c√≥ th√¨ h√£y th·ª±c hi·ªán google search, ƒë·ª´ng t·ª± t·∫°o ra th√¥ng tin. N·∫øu c√¢u h·ªèi l·∫°c ƒë·ªÅ, h√£y nh·∫•n m·∫°nh l·∫°i vai tr√≤ c·ªßa b·∫°n v√† d·∫´n d·∫Øt ng∆∞·ªùi d√πng h·ªèi nh·ªØng c√¢u h·ªèi li√™n quan"""
    
    system_parts_for_config = [google_genai_types.Part.from_text(text=system_instruction_string)]

    gemini_contents = []
    for msg in existing_chat_history:
        role = "user" if msg["role"] == "user" else "model"
        msg_content_str = str(msg.get("content", "")) 
        gemini_contents.append(google_genai_types.Content(role=role, parts=[google_genai_types.Part.from_text(text=msg_content_str)]))

    current_user_parts = [google_genai_types.Part.from_text(text=user_prompt_text)]

    session_info = next((s for s in st.session_state.get("sessions_list", []) if s["id"] == current_session_id), None)
    pdfs_already_uploaded_for_session = False
    if session_info: pdfs_already_uploaded_for_session = session_info.get("pdfs_uploaded", 0) == 1
    else:
        conn = sqlite3.connect(DATABASE_PATH); cursor = conn.cursor()
        cursor.execute("SELECT pdfs_uploaded FROM sessions WHERE id = ?", (current_session_id,))
        db_row = cursor.fetchone(); conn.close()
        if db_row: pdfs_already_uploaded_for_session = db_row[0] == 1
    
    needs_pdf_upload_this_turn = not pdfs_already_uploaded_for_session

    if needs_pdf_upload_this_turn:
        st.info("Tin nh·∫Øn ƒë·∫ßu/file ch∆∞a up cho phi√™n n√†y. ƒê√≠nh k√®m PDFs...")
        pdf_file_objects_for_this_turn = [] 
        if current_session_id not in UPLOADED_FILES_CACHE or not UPLOADED_FILES_CACHE[current_session_id]:
            pdf_file_objects_for_this_turn = upload_files_to_gemini(client, PDF_FILENAMES, current_session_id)
        else:
            pdf_file_objects_for_this_turn = UPLOADED_FILES_CACHE[current_session_id]; st.info("D√πng PDF cache cho Gemini.")
        
        if pdf_file_objects_for_this_turn:
            for file_obj in pdf_file_objects_for_this_turn:
                file_part = google_genai_types.Part(
                    file_data=google_genai_types.FileData(
                        mime_type=file_obj.mime_type, file_uri=file_obj.uri
                    ))
                current_user_parts.append(file_part)
            st.success(f"ƒê√£ chu·∫©n b·ªã {len(pdf_file_objects_for_this_turn)} PDF parts ƒë·ªÉ ƒë√≠nh k√®m.")
        else: st.warning("Kh√¥ng PDF n√†o ƒë∆∞·ª£c chu·∫©n b·ªã ƒë·ªÉ ƒë√≠nh k√®m.")
            
    elif current_session_id in UPLOADED_FILES_CACHE and UPLOADED_FILES_CACHE[current_session_id]:
        st.info("ƒê√≠nh k√®m l·∫°i c√°c file PDF ƒë√£ upload v√†o prompt (t·ª´ cache).")
        pdf_file_objects_from_cache = UPLOADED_FILES_CACHE[current_session_id]
        for file_obj in pdf_file_objects_from_cache:
            file_part = google_genai_types.Part(
                file_data=google_genai_types.FileData(
                    mime_type=file_obj.mime_type, file_uri=file_obj.uri
                ))
            current_user_parts.append(file_part)
        if pdf_file_objects_from_cache : st.success(f"ƒê√£ chu·∫©n b·ªã {len(pdf_file_objects_from_cache)} PDF parts t·ª´ cache.")

    gemini_contents.append(google_genai_types.Content(role="user", parts=current_user_parts))
    tools_for_gemini = [google_genai_types.Tool(google_search=google_genai_types.GoogleSearch())]
    
    generation_config_for_stream = google_genai_types.GenerateContentConfig(
        tools=tools_for_gemini,
        response_mime_type="text/plain",
        system_instruction=system_parts_for_config 
    )
    
    full_response_text = ""; captured_grounding_metadata_dict = None; raw_tool_calls_from_stream = []
    try:
        st.info(f"G·ªçi Gemini API ({model_to_use}) v·ªõi stream...")
        response_stream = client.models.generate_content_stream(
            model=model_to_use, contents=gemini_contents, config=generation_config_for_stream,
        )
        placeholder = st.empty()
        for chunk in response_stream: 
            if hasattr(chunk, 'text') and chunk.text: 
                full_response_text += chunk.text
                placeholder.markdown(full_response_text + "‚ñå")
            
            # Correctly check for function calls
            if hasattr(chunk, 'candidates') and chunk.candidates:
                for candidate in chunk.candidates:
                    if hasattr(candidate, 'content') and candidate.content and \
                       hasattr(candidate.content, 'parts') and candidate.content.parts:
                        for part in candidate.content.parts:
                            if hasattr(part, 'function_call') and part.function_call:
                                fc = part.function_call
                                args_dict = {}
                                if hasattr(fc, 'args') and fc.args:
                                    try: args_dict = dict(fc.args)
                                    except TypeError: 
                                        args_dict = {"error": "Could not parse fc.args to dict"}
                                        st.warning(f"Kh√¥ng th·ªÉ convert fc.args sang dict: {type(fc.args)}")
                                raw_tool_calls_from_stream.append({"name": fc.name, "args": args_dict})
                                st.caption(f"Gemini ƒë·ªÅ xu·∫•t d√πng tool: {fc.name} v·ªõi args: {args_dict}")
        placeholder.markdown(full_response_text)

        if any(tc['name'].lower() in ['googlesearch', 'google_search'] for tc in raw_tool_calls_from_stream):
            st.info("Google Search ƒë∆∞·ª£c Gemini s·ª≠ d·ª•ng (chi ti·∫øt metadata ƒë·∫ßy ƒë·ªß c·∫ßn non-streaming call).")
            search_queries = []
            for tc in raw_tool_calls_from_stream:
                if tc['name'].lower() in ['googlesearch', 'google_search'] and tc.get('args'):
                    query_arg = tc['args'].get('query', tc['args'].get('q', 'Kh√¥ng r√µ query'))
                    search_queries.append(str(query_arg))
            captured_grounding_metadata_dict = {"search_performed": True, "queries_used_by_gemini": search_queries if search_queries else ["Kh√¥ng r√µ query c·ª• th·ªÉ."]}
        
        return full_response_text, captured_grounding_metadata_dict
    except GoogleAPIError as e:
        st.error(f"L·ªói API t·ª´ Gemini: {getattr(e, 'message', str(e))} (Code: {getattr(e, 'code', 'N/A')})")
        if hasattr(e, 'summary'): st.error(f"T√≥m t·∫Øt l·ªói: {getattr(e, 'summary', '')}")
        return f"[L·ªói Gemini API: {getattr(e, 'message', str(e))}]", None
    except Exception as e: 
        st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi Gemini API: {e}")
        return f"[L·ªói Gemini: {e}]", None

# --- Streamlit UI ---
st.set_page_config(page_title="Chatbot GTCC HCM (Gemini)", layout="wide")
if "current_session_id" not in st.session_state: st.session_state.current_session_id = None
if "chat_history" not in st.session_state: st.session_state.chat_history = []
if "sessions_list" not in st.session_state: st.session_state.sessions_list = get_sessions_db()
if "gemini_api_key" not in st.session_state: st.session_state.gemini_api_key = load_api_key()

if st.session_state.gemini_api_key and GEMINI_CLIENT is None:
    GEMINI_CLIENT = get_gemini_client(st.session_state.gemini_api_key)

with st.sidebar:
    st.header("Phi√™n tr√≤ chuy·ªán")
    if st.button("‚ûï Tr√≤ chuy·ªán m·ªõi", use_container_width=True):
        new_id, _ = create_new_session_db(); st.session_state.current_session_id = new_id
        st.session_state.chat_history = []; 
        if new_id in UPLOADED_FILES_CACHE: del UPLOADED_FILES_CACHE[new_id] 
        st.session_state.sessions_list = get_sessions_db(); st.rerun()
    
    st.session_state.sessions_list = get_sessions_db() # Refresh list
    if not st.session_state.current_session_id and st.session_state.sessions_list:
        st.session_state.current_session_id = st.session_state.sessions_list[0]["id"]
        st.session_state.chat_history = load_messages_db(st.session_state.current_session_id)
    
    for session_item in st.session_state.sessions_list:
        cols = st.columns([0.7, 0.15, 0.15]); 
        is_curr = st.session_state.current_session_id == session_item['id']
        btn_label = f"{'‚û°Ô∏è ' if is_curr else ''}{session_item['name']}"
        if cols[0].button(btn_label, key=f"session_{session_item['id']}", use_container_width=True):
            if not is_curr: 
                st.session_state.current_session_id = session_item['id']; 
                st.session_state.chat_history = load_messages_db(session_item['id']); 
                st.rerun()
        if cols[1].button("‚úèÔ∏è", key=f"rename_{session_item['id']}", help="ƒê·ªïi t√™n"): 
            st.session_state.renaming_session_id = session_item['id']; st.rerun()
        if cols[2].button("üóëÔ∏è", key=f"delete_{session_item['id']}", help="Xo√°"):
            if delete_session_db(session_item['id']):
                if st.session_state.current_session_id == session_item['id']: 
                    st.session_state.current_session_id = None; st.session_state.chat_history = []
                if session_item['id'] in UPLOADED_FILES_CACHE: del UPLOADED_FILES_CACHE[session_item['id']]
                st.session_state.sessions_list = get_sessions_db(); st.rerun()
                
    if st.session_state.get('renaming_session_id'):
        with st.form(key="rename_form"):
            st.subheader("ƒê·ªïi t√™n tr√≤ chuy·ªán")
            current_name_for_rename = next((s['name'] for s in st.session_state.sessions_list if s['id'] == st.session_state.renaming_session_id), "")
            new_session_name = st.text_input("T√™n m·ªõi:", value=current_name_for_rename)
            if st.form_submit_button("L∆∞u"):
                if new_session_name.strip():
                    if rename_session_db(st.session_state.renaming_session_id, new_session_name.strip()):
                        del st.session_state.renaming_session_id; 
                        st.session_state.sessions_list = get_sessions_db(); st.rerun()
                else: 
                    st.warning("T√™n kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng.")
    st.divider()
    st.header("C√†i ƒë·∫∑t API Gemini")
    if st.session_state.gemini_api_key:
        st.success("ƒê√£ c√≥ Gemini API Key.")
        if st.button("Thay ƒë·ªïi/X√≥a API Key"): 
            st.session_state.gemini_api_key = None; save_api_key(None); 
            GEMINI_CLIENT = None; st.rerun()
    else:
        new_key = st.text_input("Nh·∫≠p Gemini API Key:", type="password", key="new_gem_key_input")
        if st.button("L∆∞u API Key", key="save_gem_key_btn"):
            if new_key: 
                client_test = get_gemini_client(new_key) # Test key
                if client_test: 
                    st.session_state.gemini_api_key = new_key; save_api_key(new_key); 
                    GEMINI_CLIENT = client_test; st.success("ƒê√£ l∆∞u!"); st.rerun()
            else: 
                st.warning("Vui l√≤ng nh·∫≠p API Key.")

st.title("Chatbot GTCC TP.HCM (Gemini API)")
current_session_name = "Ch∆∞a ch·ªçn phi√™n"
if st.session_state.current_session_id:
    cs_info = next((s for s in st.session_state.sessions_list if s["id"] == st.session_state.current_session_id), None)
    if cs_info: current_session_name = cs_info["name"]
st.subheader(f"Phi√™n: {current_session_name}")

if st.session_state.current_session_id:
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("gemini_grounding_metadata"):
                meta = msg["gemini_grounding_metadata"]
                with st.expander("Th√¥ng tin t√¨m ki·∫øm Google (t·ª´ Gemini)", expanded=False):
                    if meta.get("search_performed"): st.caption("Gemini ƒë√£ s·ª≠ d·ª•ng Google Search.")
                    if meta.get("queries_used_by_gemini"): st.write("Truy v·∫•n c√≥ th·ªÉ ƒë√£ d√πng:", meta.get("queries_used_by_gemini"))
                    if not meta.get("queries_used_by_gemini") and meta.get("search_performed"): st.write("Kh√¥ng c√≥ chi ti·∫øt truy v·∫•n t·ª´ stream.")
                    elif not meta.get("search_performed"): st.write("Kh√¥ng c√≥ t√¨m ki·∫øm n√†o ƒë∆∞·ª£c th·ª±c hi·ªán.")


user_prompt = st.chat_input("C√¢u h·ªèi v·ªÅ giao th√¥ng c√¥ng c·ªông TP.HCM:")
if user_prompt and st.session_state.current_session_id:
    if not GEMINI_CLIENT: st.error("Client Gemini ch∆∞a s·∫µn s√†ng. Ki·ªÉm tra API Key.")
    else:
        st.session_state.chat_history.append({"role": "user", "content": user_prompt})
        save_message_db(st.session_state.current_session_id, "user", user_prompt)
        with st.chat_message("user"): st.markdown(user_prompt)
        with st.chat_message("assistant"):
            full_response, grounding_meta_dict = generate_gemini_response_stream(
                GEMINI_CLIENT, user_prompt, st.session_state.current_session_id,
                st.session_state.chat_history[:-1] 
            )
            assistant_msg_obj = {"role": "assistant", "content": full_response}
            if grounding_meta_dict: assistant_msg_obj["gemini_grounding_metadata"] = grounding_meta_dict
            st.session_state.chat_history.append(assistant_msg_obj)
            save_message_db(st.session_state.current_session_id, "assistant", full_response, grounding_metadata_obj=grounding_meta_dict)
elif user_prompt and not st.session_state.current_session_id:
    st.warning("Vui l√≤ng ch·ªçn ho·∫∑c t·∫°o phi√™n tr√≤ chuy·ªán m·ªõi.")
